import re
import difflib
from pathlib import Path
from collections import defaultdict
import spacy
import ahocorasick
from kiwipiepy import Kiwi
from typing import List, Tuple, Optional

class TweeL10nValidator:
    """
    .twee íŒŒì¼ì˜ ë¡œì»¬ë¼ì´ì œì´ì…˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³ , ì˜ˆì¸¡ ê°€ëŠ¥í•œ êµ¬ë¬¸ ì˜¤ë¥˜ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ëŠ” ì¢…í•© í´ë˜ìŠ¤.
    'ë¼ì¸ ìœ í˜• ë¶„ë¥˜ê¸°'ì™€ 'ë§¤í¬ë¡œ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜¤íƒì„ ìµœì†Œí™”í•˜ê³  ì •í™•ë„ë¥¼ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
    """

    # --- ìƒìˆ˜ ì •ì˜ ---
    UNTRANSLATED_LINK_WORD_THRESHOLD = 4
    ENGLISH_RATIO_THRESHOLD = 0.8
    CONTEXT_LINES = 2

    # --- ì •ê·œí‘œí˜„ì‹ ---
    REGEX = {
        "passage_header": re.compile(r"^(::\s.*)$"),
        "macro": re.compile(r"<<.*?>>"),
        "macro_name": re.compile(r"<<\s*([a-zA-Z0-9_]+)"),
        "variable": re.compile(r"\$[a-zA-Z0-9_.]+"),
        "link_with_dest": re.compile(r"\[\[(.*?)\|(.*?)\]\]"),
        "link_simple": re.compile(r"\[\[(.*?)\]\]"),
        "link": re.compile(r"\[\[.*?\]\]"),
        "string_literal": re.compile(r'["\'](.*?)["\']'),
        "korean": re.compile(r"[ê°€-í£]"),
        "english_only": re.compile(r"^[a-zA-Z\s.,!?'\"():<>_`~@#$%^&*=\[\]{}|\\/+-]+$"),
        "word_tokenizer": re.compile(r"[\w']+"),
        "corrupted_char": re.compile(r"ï¿½"),
        "forbidden_pattern": re.compile(r"[ê°€-í£]+\s*\([A-Za-z\s]+\)"),
        "markdown_header": re.compile(r"^(#+)\s.*$"),
        "html_tag": re.compile(r"<.*?>"),
        "comment": re.compile(r"^\s*(/\*.*?\*/|<!--.*?-->)"),
    }
    REGEX["code_block"] = re.compile(f"({REGEX['macro'].pattern}|{REGEX['link'].pattern}|{REGEX['variable'].pattern}|{REGEX['html_tag'].pattern})")

    # --- í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ / ë¸”ë™ë¦¬ìŠ¤íŠ¸ ---
    ALLOWED_POSTPOSITIONS = frozenset([
        "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ê³¼", "ì™€", "ì˜", "ê»˜", "ì—ê²Œ", "í•œí…Œ",
        "ìœ¼ë¡œ", "ë¡œ", "ì—ì„œ", "ë¶€í„°", "ê¹Œì§€", "ë§Œ", "ë„", "ë¿", "ì´ë¼", "ë¼",
        "ì´ì—¬", "ì—¬", "ì´ì‹œì—¬", "ì‹œì—¬", "ì•„", "ì•¼"
    ])
    TEXT_MACROS_WHITELIST = frozenset([
        "print", "say", "either", "display", "link", "button", "checkbox",
        "radiobutton", "textbox", "textarea", "timed", "repeat",
        "HePost", "bHePost", "nnpc_HePost", "putpost", "sextoyPost"
    ])

    def __init__(self, original_path: Path, translated_path: Path, glossary_path: Optional[Path]):
        print("ê²€ì¦ê¸° ì´ˆê¸°í™” ì¤‘... (NLP ëª¨ë¸ ë° ìš©ì–´ì§‘ ë¡œë”©)")
        self.original_path = original_path
        self.translated_path = translated_path
        self.glossary_path = glossary_path
        self.issues = []

        self.nlp_en = spacy.load("en_core_web_sm")
        self.kiwi = Kiwi()
        self.glossary_automaton = ahocorasick.Automaton()

        self._load_files()
        self._build_glossary_automaton()
        print("ì´ˆê¸°í™” ì™„ë£Œ.")

    def _load_files(self):
        try:
            if not hasattr(self, 'original_lines'):
                self.original_lines = self.original_path.read_text('utf-8').splitlines()
            self.translated_lines = self.translated_path.read_text('utf-8').splitlines()
        except FileNotFoundError as e:
            print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {e}")
            exit(1)

    def _build_glossary_automaton(self):
        self.glossary = {}
        if self.glossary_path and self.glossary_path.exists():
            lines = self.glossary_path.read_text('utf-8').splitlines()
            for i, line in enumerate(lines):
                if line.strip().startswith('#') or not line.strip() or ':' not in line:
                    continue
                parts = [p.strip() for p in line.split(':', 1)]
                if len(parts) == 2 and parts[0] and parts[1]:
                    eng_key, kor_value = parts[0], parts[1]
                    keys_to_add = {eng_key, eng_key.lower(), eng_key.capitalize()}
                    for key in keys_to_add:
                        if key not in self.glossary:
                            self.glossary[key] = kor_value
                            self.glossary_automaton.add_word(key, (key, kor_value))
        
        self.glossary_automaton.make_automaton()
        if not self.glossary and self.glossary_path:
             print(f"ê²½ê³ : ìš©ì–´ì§‘ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: '{self.glossary_path}'")

    def _add_issue(self, **kwargs):
        self.issues.append(kwargs)

    def _get_pure_text(self, line: str) -> str:
        return self.REGEX["code_block"].sub("", line)

    def _classify_line(self, line: str) -> str:
        stripped_line = line.strip()
        if not stripped_line: return "BLANK"
        if self.REGEX["passage_header"].match(line): return "PASSAGE_HEADER"
        if self.REGEX["markdown_header"].match(line): return "MARKDOWN_HEADER"
        if self.REGEX["comment"].match(stripped_line): return "COMMENT"
        
        pure_text = self._get_pure_text(line).strip()
        has_code = self.REGEX["code_block"].search(line)

        if has_code and not pure_text: return "PURE_CODE"
        if not has_code and pure_text: return "PURE_TEXT"
        if has_code and pure_text: return "MIXED_CONTENT"
        
        return "UNKNOWN_CODE"

    def run_all_checks(self):
        print("\n--- 1ë‹¨ê³„: êµ¬ì¡°ì  ë¬´ê²°ì„± ê²€ì‚¬ ì‹œì‘ ---")
        is_structurally_sound = self._check_line_count_and_structure()
        self._check_core_identifiers()
        
        print("\n--- 2ë‹¨ê³„: êµ¬ë¬¸, í”Œë ˆì´ ê°€ëŠ¥ì„±, ê·œì¹™ ì¤€ìˆ˜ ê²€ì‚¬ ì‹œì‘ ---")
        self._check_global_variable_consistency()
        self._check_all_lines(is_structurally_sound)
        
        print(f"\nëª¨ë“  ê²€ì¦ ì™„ë£Œ. ì´ {len(self.issues)}ê°œì˜ ë¬¸ì œ ë°œê²¬.")

    def _check_line_count_and_structure(self) -> bool:
        if len(self.original_lines) == len(self.translated_lines):
            return True
        self._add_issue(
            line_num=0, severity="CRITICAL", type="êµ¬ì¡°ì  ì˜¤ë¥˜",
            description=f"íŒŒì¼ì˜ ì „ì²´ ì¤„ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì›ë³¸: {len(self.original_lines)}ì¤„, ë²ˆì—­ë³¸: {len(self.translated_lines)}ì¤„)"
        )
        matcher = difflib.SequenceMatcher(None, self.original_lines, self.translated_lines, autojunk=False)
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal' or tag == 'replace': continue
            diff_lines = []
            start = max(0, i1 - self.CONTEXT_LINES)
            for i in range(start, i1): diff_lines.append(f"  {self.original_lines[i]}")
            description = ""
            if tag == 'delete':
                description = f"ì›ë³¸ {i1+1}ë²ˆì§¸ ì¤„ ê·¼ì²˜ì˜ ë‚´ìš©ì´ ë²ˆì—­ë³¸ì—ì„œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                for i in range(i1, i2): diff_lines.append(f"- {self.original_lines[i]}")
            elif tag == 'insert':
                description = f"ë²ˆì—­ë³¸ {j1+1}ë²ˆì§¸ ì¤„ ê·¼ì²˜ì— ì›ë³¸ì— ì—†ëŠ” ë‚´ìš©ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
                for j in range(j1, j2): diff_lines.append(f"+ {self.translated_lines[j]}")
            end = min(len(self.original_lines), i2 + self.CONTEXT_LINES)
            for i in range(i2, end): diff_lines.append(f"  {self.original_lines[i]}")
            self._add_issue(
                line_num=i1 + 1, severity="CRITICAL", type="êµ¬ì¡°ì  ë¶ˆì¼ì¹˜ (ì¤„ ì‚­ì œ/ì¶”ê°€)",
                description=description, diff_text="\n".join(diff_lines)
            )
        return False

    def _check_core_identifiers(self):
        orig_headers = [h for h, l in self._extract_identifiers(self.original_lines, "passage_header")]
        trans_headers = [h for h, l in self._extract_identifiers(self.translated_lines, "passage_header")]
        if orig_headers != trans_headers:
             self._add_issue(line_num=0, severity="CRITICAL", type="íŒ¨ì‹œì§€ í—¤ë” ë¶ˆì¼ì¹˜",
                             description="íŒ¨ì‹œì§€ í—¤ë”ì˜ ìˆœì„œë‚˜ ë‚´ìš©ì´ ì›ë³¸ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ê²Œì„ ë§í¬ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        orig_dests = {d for d, l in self._extract_identifiers(self.original_lines, "link_destination")}
        trans_dests = {d for d, l in self._extract_identifiers(self.translated_lines, "link_destination")}
        if orig_dests != trans_dests:
            missing = orig_dests - trans_dests
            added = trans_dests - orig_dests
            desc = "ë§í¬ ëª©ì ì§€ ëª©ë¡ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            if missing: desc += f" ëˆ„ë½: {', '.join(sorted(list(missing))[:5])} ë“±"
            if added: desc += f" ì¶”ê°€/ì˜¤íƒ€: {', '.join(sorted(list(added))[:5])} ë“±"
            self._add_issue(line_num=0, severity="CRITICAL", type="ë§í¬ ëª©ì ì§€ ë¶ˆì¼ì¹˜", description=desc)

    def _extract_identifiers(self, lines: List[str], id_type: str) -> List[Tuple[str, int]]:
        extracted = []
        for i, line in enumerate(lines):
            if id_type == "passage_header":
                if match := self.REGEX["passage_header"].match(line):
                    extracted.append((match.group(1).strip(), i + 1))
            elif id_type == "link_destination":
                for _, dest in self.REGEX["link_with_dest"].findall(line):
                    extracted.append((dest.strip(), i + 1))
                for dest in self.REGEX["link_simple"].findall(line):
                    if '|' not in dest: extracted.append((dest.strip(), i + 1))
        return extracted

    def _check_global_variable_consistency(self):
        original_vars = set(self.REGEX["variable"].findall("\n".join(self.original_lines)))
        translated_vars = set(self.REGEX["variable"].findall("\n".join(self.translated_lines)))
        if original_vars != translated_vars:
            missing = original_vars - translated_vars
            added = translated_vars - original_vars
            desc = "ì „ì²´ ë³€ìˆ˜ ëª©ë¡ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            if missing: desc += f" ëˆ„ë½ëœ ë³€ìˆ˜: {', '.join(sorted(list(missing))[:5])} ë“±"
            if added: desc += f" ì¶”ê°€/ì†ìƒëœ ë³€ìˆ˜: {', '.join(sorted(list(added))[:5])} ë“±"
            self._add_issue(line_num=0, severity="CRITICAL", type="ì „ì—­ ë³€ìˆ˜ ë¶ˆì¼ì¹˜", description=desc)

    def _check_all_lines(self, is_structurally_sound: bool):
        for i, translated_line in enumerate(self.translated_lines):
            line_num = i + 1
            original_line = self.original_lines[i] if is_structurally_sound else ""
            line_type = self._classify_line(translated_line)

            if line_type in ["PURE_TEXT", "MIXED_CONTENT"]:
                self._check_links_for_playability(translated_line, line_num)
                self._check_untranslated_content(original_line, translated_line, line_num, is_structurally_sound)
                self._check_forbidden_patterns(translated_line, line_num)
                if is_structurally_sound:
                    self._check_glossary_compliance_nlp(original_line, translated_line, line_num)
            
            if is_structurally_sound and line_type in ["PURE_CODE", "MIXED_CONTENT", "UNKNOWN_CODE"]:
                self._check_macro_corruption(original_line, translated_line, line_num)

            self._check_text_corruption(translated_line, line_num)

    def _check_macro_corruption(self, original_line, translated_line, line_num):
        original_macros = self.REGEX["macro"].findall(original_line)
        translated_macros = self.REGEX["macro"].findall(translated_line)
        if len(original_macros) == len(translated_macros):
            for orig_macro, trans_macro in zip(original_macros, translated_macros):
                match = self.REGEX["macro_name"].match(trans_macro)
                if not match: continue
                
                macro_name = match.group(1)
                if macro_name not in self.TEXT_MACROS_WHITELIST:
                    content = trans_macro[2:-2].strip()
                    literals = self.REGEX["string_literal"].findall(content)
                    for literal in literals:
                        if self.REGEX["korean"].search(literal) and literal not in self.ALLOWED_POSTPOSITIONS:
                            self._add_issue(
                                severity="CRITICAL", type="ë§¤í¬ë¡œ ì½”ë“œ ì†ìƒ",
                                description=f"ë²ˆì—­ ê¸ˆì§€ ì˜ì‹¬ ë§¤í¬ë¡œ(`{macro_name}`) ë‚´ë¶€ì˜ ì½”ë“œ ì‹ë³„ì '{literal}'ì´(ê°€) ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤.",
                                line_num=line_num, original=f"`{orig_macro}`", translated=f"`{trans_macro}`"
                            )
                            break

    def _check_links_for_playability(self, line, line_num):
        all_links = self.REGEX["link_with_dest"].findall(line) + [(m, m) for m in self.REGEX["link_simple"].findall(line) if '|' not in m]
        for display_text, dest in all_links:
            pure_display_text = self._get_pure_text(display_text)
            if not pure_display_text.strip():
                self._add_issue(severity="WARNING", type="ë¹ˆ ìƒí˜¸ì‘ìš©",
                                description="í”Œë ˆì´ì–´ê°€ í´ë¦­í•  ìˆ˜ ì—†ëŠ” 'ë¹ˆ ë§í¬'ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                                line_num=line_num, translated=line)
            elif self.REGEX["english_only"].match(pure_display_text):
                word_count = len(self.REGEX["word_tokenizer"].findall(pure_display_text))
                if word_count > 0:
                    severity = "WARNING" if word_count >= self.UNTRANSLATED_LINK_WORD_THRESHOLD else "INFO"
                    self._add_issue(severity=severity, type="ë¯¸ë²ˆì—­ ì˜ì‹¬ (ë§í¬)",
                                    description=f"ë§í¬ í‘œì‹œ í…ìŠ¤íŠ¸ '{display_text}'ì´(ê°€) ë²ˆì—­ë˜ì§€ ì•Šì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
                                    line_num=line_num, translated=line)

    def _check_untranslated_content(self, original_line, translated_line, line_num, is_structurally_sound):
        pure_translated = self._get_pure_text(translated_line)
        if not pure_translated.strip() or self.REGEX["korean"].search(pure_translated):
            return
        
        words = self.REGEX["word_tokenizer"].findall(pure_translated)
        if not words: return

        english_words = sum(1 for word in words if self.REGEX["english_only"].match(word) and not word.isdigit())
        if (english_words / len(words)) >= self.ENGLISH_RATIO_THRESHOLD:
            severity = "WARNING" if is_structurally_sound and original_line.strip() == translated_line.strip() else "INFO"
            desc = "ì´ ë¼ì¸ì€ ë²ˆì—­ì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜(ì›ë³¸ê³¼ ë™ì¼), ëŒ€ë¶€ë¶„ì´ ì˜ì–´ë¡œ êµ¬ì„±ë˜ì–´ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            self._add_issue(
                severity=severity, type="ë¯¸ë²ˆì—­ ì˜ì‹¬ (ì½˜í…ì¸ )", description=desc,
                line_num=line_num, original=original_line, translated=translated_line
            )

    def _check_text_corruption(self, line, line_num):
        if self.REGEX["corrupted_char"].search(line):
            self._add_issue(
                severity="CRITICAL", type="í…ìŠ¤íŠ¸ ì†ìƒ",
                description="íŒŒì¼ ì¸ì½”ë”© ë¬¸ì œë¡œ ì¸í•´ ê¹¨ì§„ ë¬¸ì(ï¿½)ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                line_num=line_num, translated=line
            )

    def _check_forbidden_patterns(self, line, line_num):
        if self.REGEX["forbidden_pattern"].search(line):
            self._add_issue(
                severity="WARNING", type="ê¸ˆì§€ëœ íŒ¨í„´ ì‚¬ìš©",
                description="ë²ˆì—­ë¬¸ ë’¤ì— ê´„í˜¸ë¥¼ ì‚¬ìš©í•œ ì›ë¬¸ ë³‘ê¸° íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                line_num=line_num, translated=line
            )

    def _check_glossary_compliance_nlp(self, original_line, translated_line, line_num):
        if not self.glossary: return
        pure_original = self._get_pure_text(original_line)
        if not pure_original.strip(): return
        found_eng_terms = {item[1][0] for item in self.glossary_automaton.iter(pure_original)}
        if not found_eng_terms: return
        pure_translated = self._get_pure_text(translated_line)
        tokens = self.kiwi.tokenize(pure_translated)
        found_kor_tokens = {token.form for token in tokens}
        for eng_key in found_eng_terms:
            kor_value = self.glossary.get(eng_key)
            if not kor_value: continue
            if kor_value not in found_kor_tokens and kor_value not in pure_translated:
                if eng_key.lower() in pure_translated.lower():
                    self._add_issue(
                        severity="INFO", type="ìš©ì–´ì§‘ ë¯¸ì ìš©",
                        description=f"ìš©ì–´ì§‘ ë‹¨ì–´ '{eng_key}'ê°€ ë²ˆì—­ë˜ì§€ ì•Šê³  ì›ë¬¸ì— ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.",
                        line_num=line_num, original=original_line, translated=translated_line
                    )
                else:
                    self._add_issue(
                        severity="WARNING", type="ìš©ì–´ì§‘ ì˜¤ì—­/ëˆ„ë½ ì˜ì‹¬",
                        description=f"ìš©ì–´ì§‘ ë‹¨ì–´ '{eng_key}'ì˜ ë²ˆì—­ '{kor_value}'ì´(ê°€) ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ë²ˆì—­ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
                        line_num=line_num, original=original_line, translated=translated_line
                    )

    def generate_report(self, output_path: Path, report_title: str):
        if not self.issues:
            report_content = f"# âœ… {report_title}: {self.translated_path.name}\n\n**ì¶•í•˜í•©ë‹ˆë‹¤! ë°œê²¬ëœ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.**"
        else:
            severity_order = {"CRITICAL": 0, "WARNING": 1, "INFO": 2}
            sorted_issues = sorted(self.issues, key=lambda x: (severity_order.get(x["severity"], 99), x["line_num"]))
            
            summary = defaultdict(int)
            for issue in self.issues: summary[issue["severity"]] += 1

            report_content = f"# â— {report_title}: {self.translated_path.name}\n\n"
            report_content += f"## ìš”ì•½\n\n- **ì´ ë¬¸ì œ ìˆ˜: {len(self.issues)}**\n"
            if summary["CRITICAL"] > 0: report_content += f"- ğŸ”´ **ì¹˜ëª…ì  ì˜¤ë¥˜ (CRITICAL): {summary['CRITICAL']}**\n"
            if summary["WARNING"] > 0: report_content += f"- ğŸŸ¡ **ê²½ê³  (WARNING): {summary['WARNING']}**\n"
            if summary["INFO"] > 0: report_content += f"- ğŸ”µ **ì •ë³´ (INFO): {summary['INFO']}**\n"
            
            report_content += "\n---\n\n## ìƒì„¸ ë‚´ìš©\n\n"

            for issue in sorted_issues:
                icon = {"CRITICAL": "ğŸ”´", "WARNING": "ğŸŸ¡", "INFO": "ğŸ”µ"}.get(issue["severity"], "âšªï¸")
                line_info = f"(ì›ë³¸ ê¸°ì¤€ Line: {issue['line_num']})" if issue['line_num'] > 0 else "(ì „ì—­ ê²€ì‚¬)"
                report_content += f"### {icon} [{issue['severity']}] {issue['type']} {line_info}\n\n"
                report_content += f"- **ë¬¸ì œ ì„¤ëª…:** {issue['description']}\n"
                if issue.get('diff_text'):
                    report_content += f"\n**ì°¨ì´ì  ë¶„ì„ (Diff):**\n```diff\n{issue['diff_text']}\n```\n"
                if issue.get('original'):
                    report_content += f"- **ì›ë³¸:** `{issue.get('original')}`\n"
                if issue.get('translated'):
                    report_content += f"- **ë²ˆì—­ë³¸:** `{issue.get('translated')}`\n"
                report_content += "\n---\n"
        
        output_path.write_text(report_content, 'utf-8')
        print(f"\në¦¬í¬íŠ¸ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- ìë™ ìˆ˜ì • ê¸°ëŠ¥ ---
    def run_auto_fixer(self, output_path: Path):
        """ì•Œë ¤ì§„ ê·œì¹™ì ì¸ êµ¬ë¬¸ ì˜¤ë¥˜ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ê³  ìƒˆ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        print("\n--- ìë™ ìˆ˜ì • ì‘ì—… ì‹œì‘ ---")
        
        fix_patterns = [
            # íŒ¨í„´ 1: <<macro arg_ì¡°ì‚¬>> -> <<macro_ì¡°ì‚¬ arg>>
            (re.compile(r'<<([a-zA-Z0-9_]+)((?:\s+(?:[0-9]+|"[^"]+"|\$[a-zA-Z0-9_.]+))+)(_\s*[ê°€-í£]+)>>'), r'<<\1\3\2>>'),
            # íŒ¨í„´ 2: <</macro_ì¡°ì‚¬>> -> <</macro>>ì¡°ì‚¬
            (re.compile(r'(<</[a-zA-Z0-9_]+)_(\s*[ê°€-í£]+)>>'), r'\1>>\2'),
            # íŒ¨í„´ 3: </if>> -> <</if>> (ì•ˆì „í•œ íŒ¨í„´)
            (re.compile(r'(?<!<)</if>>'), r'<</if>>'),
        ]

        fixed_lines = []
        fixes = []
        
        for i, line in enumerate(self.translated_lines):
            modified_line = line
            for pattern, substitution in fix_patterns:
                modified_line = pattern.sub(substitution, modified_line)
            
            if modified_line != line:
                fixes.append({
                    "line_num": i + 1,
                    "original": line,
                    "translated": modified_line
                })
            fixed_lines.append(modified_line)

        output_path.write_text("\n".join(fixed_lines), 'utf-8')
        print(f"ìë™ ìˆ˜ì •ëœ íŒŒì¼ì´ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        self._generate_fix_report(fixes)
        
        return output_path

    def _generate_fix_report(self, fixes: List[dict]):
        report_path = self.translated_path.with_name(self.translated_path.stem + "_fix_report.md")
        if not fixes:
            report_content = f"# âœ… ìë™ ìˆ˜ì • ë¦¬í¬íŠ¸: {self.translated_path.name}\n\nìˆ˜ì •í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤."
        else:
            report_content = f"# ğŸ› ï¸ ìë™ ìˆ˜ì • ë¦¬í¬íŠ¸: {self.translated_path.name}\n\n"
            report_content += f"ì´ **{len(fixes)}**ê°œì˜ ë¼ì¸ì—ì„œ êµ¬ë¬¸ ì˜¤ë¥˜ê°€ ìë™ìœ¼ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n---\n\n"
            for fix in fixes:
                report_content += f"### Line: {fix['line_num']}\n"
                report_content += f"**ì›ë³¸:**\n```twee\n{fix['original']}\n```\n"
                report_content += f"**ìˆ˜ì •ë³¸:**\n```twee\n{fix['translated']}\n```\n\n---\n"
        
        report_path.write_text(report_content, 'utf-8')
        print(f"ìë™ ìˆ˜ì • ë¦¬í¬íŠ¸ê°€ '{report_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    # --- ì„¤ì •: ì—¬ê¸°ì— ê²€ì¦í•  íŒŒì¼ ê²½ë¡œë¥¼ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”. ---
    ORIGINAL_FILE_PATH =
    TRANSLATED_FILE_PATH =
    GLOSSARY_FILE_PATH =
    
    # ìë™ ìˆ˜ì • ë° ê²€ì¦ ê²°ê³¼ íŒŒì¼ ì´ë¦„ ì„¤ì •
    FIXED_TRANSLATED_FILE_PATH = Path(TRANSLATED_FILE_PATH).with_name(Path(TRANSLATED_FILE_PATH).stem + "_fixed.txt")
    VALIDATION_REPORT_PATH = 
    # ----------------------------------------------------

    original_p = Path(ORIGINAL_FILE_PATH)
    translated_p = Path(TRANSLATED_FILE_PATH)
    glossary_p = Path(GLOSSARY_FILE_PATH) if GLOSSARY_FILE_PATH and Path(GLOSSARY_FILE_PATH).exists() else None
    fixed_translated_p = Path(FIXED_TRANSLATED_FILE_PATH)
    output_p = Path(VALIDATION_REPORT_PATH)

    # 0ë‹¨ê³„: ìë™ ìˆ˜ì •ê¸° ì‹¤í–‰
    fixer_validator = TweeL10nValidator(original_path=original_p, translated_path=translated_p, glossary_path=glossary_p)
    fixed_file_path = fixer_validator.run_auto_fixer(output_path=fixed_translated_p)

    # 1, 2, 3ë‹¨ê³„: ìˆ˜ì •ëœ íŒŒì¼ì„ ëŒ€ìƒìœ¼ë¡œ ì „ì²´ ê²€ì¦ ì‹¤í–‰
    final_validator = TweeL10nValidator(original_path=original_p, translated_path=fixed_file_path, glossary_path=glossary_p)
    final_validator.run_all_checks()
    final_validator.generate_report(output_path=output_p, report_title="ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸")
