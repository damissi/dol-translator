import re
import difflib
from pathlib import Path
from collections import defaultdict
import spacy
import ahocorasick
from kiwipiepy import Kiwi
from typing import List, Tuple, Optional, Set

class TweeL10nValidator:
    """
    .twee íŒŒì¼ì˜ ì›ë³¸ê³¼ ë²ˆì—­ë³¸ì„ ë¹„êµí•˜ì—¬ ë¡œì»¬ë¼ì´ì œì´ì…˜ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ì¢…í•© í´ë˜ìŠ¤.
    NLPì™€ Aho-Corasick ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°, êµ¬ë¬¸, í”Œë ˆì´ ê°€ëŠ¥ì„±, ê·œì¹™ ì¤€ìˆ˜ë¥¼
    ì¢…í•©ì ìœ¼ë¡œ ê²€ì‚¬í•˜ê³  ìƒì„¸í•œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """

    # --- ìƒìˆ˜ ì •ì˜ ---
    UNTRANSLATED_LINK_WORD_THRESHOLD = 4
    ENGLISH_RATIO_THRESHOLD = 0.8
    CONTEXT_LINES = 2

    # --- ì •ê·œí‘œí˜„ì‹ ---
    REGEX = {
        "passage_header": re.compile(r"^(::\s.*)$"),
        "macro": re.compile(r"<<.*?>>"),
        "variable": re.compile(r"\$[a-zA-Z0-9_.]+"),
        "link_with_dest": re.compile(r"\[\[(.*?)\|(.*?)\]\]"),
        "link_simple": re.compile(r"\[\[(.*?)\]\]"),
        "link": re.compile(r"\[\[.*?\]\]"),
        "string_literal": re.compile(r'["\'](.*?)["\']'),
        "korean": re.compile(r"[ê°€-í£]"),
        "english_only": re.compile(r"^[a-zA-Z\s.,!?'\"():<>_`~@#$%^&*=\[\]{}|\\/+-]+$"),
        "word_tokenizer": re.compile(r"[\w']+"),
        "corrupted_char": re.compile(r"ï¿½"),
        "forbidden_pattern": re.compile(r"[ê°€-í£]+\s*\([A-Za-z\s]+\)"),
        "code_only_line": re.compile(r"^\s*(<<.*>>|/\*.*?\*/|<!--.*?-->)\s*$"),
        "markdown_header": re.compile(r"^(#+)\s.*$"),
        "html_tag": re.compile(r"<.*?>"),
        "comment": re.compile(r"^(/\*.*?\*/|<!--.*?-->)"),
    }
    REGEX["code_block"] = re.compile(f"({REGEX['macro'].pattern}|{REGEX['link'].pattern}|{REGEX['variable'].pattern}|{REGEX['html_tag'].pattern})")

    # --- í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ---
    ALLOWED_POSTPOSITIONS = frozenset([
        "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ê³¼", "ì™€", "ì˜", "ê»˜", "ì—ê²Œ", "í•œí…Œ",
        "ìœ¼ë¡œ", "ë¡œ", "ì—ì„œ", "ë¶€í„°", "ê¹Œì§€", "ë§Œ", "ë„", "ë¿", "ì´ë¼", "ë¼",
        "ì´ì—¬", "ì—¬", "ì´ì‹œì—¬", "ì‹œì—¬", "ì•„", "ì•¼"
    ])

    def __init__(self, original_path: Path, translated_path: Path, glossary_path: Optional[Path]):
        print("ê²€ì¦ê¸° ì´ˆê¸°í™” ì¤‘... (NLP ëª¨ë¸ ë° ìš©ì–´ì§‘ ë¡œë”©)")
        self.original_path = original_path
        self.translated_path = translated_path
        self.glossary_path = glossary_path
        self.issues = []

        self.nlp_en = spacy.load("en_core_web_sm")
        self.kiwi = Kiwi()
        self.glossary_automaton = ahocorasick.Automaton()

        self._load_files()
        self._build_glossary_automaton()
        print("ì´ˆê¸°í™” ì™„ë£Œ.")

    def _load_files(self):
        try:
            self.original_lines = self.original_path.read_text('utf-8').splitlines()
            self.translated_lines = self.translated_path.read_text('utf-8').splitlines()
        except FileNotFoundError as e:
            print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {e}")
            exit(1)

    def _build_glossary_automaton(self):
        self.glossary = {}
        if self.glossary_path and self.glossary_path.exists():
            lines = self.glossary_path.read_text('utf-8').splitlines()
            for i, line in enumerate(lines):
                if line.strip().startswith('#') or not line.strip() or ':' not in line:
                    continue
                parts = [p.strip() for p in line.split(':', 1)]
                if len(parts) == 2 and parts[0] and parts[1]:
                    eng_key, kor_value = parts[0], parts[1]
                    keys_to_add = {eng_key, eng_key.lower(), eng_key.capitalize()}
                    for key in keys_to_add:
                        if key not in self.glossary:
                            self.glossary[key] = kor_value
                            self.glossary_automaton.add_word(key, (key, kor_value))
        
        self.glossary_automaton.make_automaton()
        if not self.glossary and self.glossary_path:
             print(f"ê²½ê³ : ìš©ì–´ì§‘ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: '{self.glossary_path}'")

    def _add_issue(self, **kwargs):
        self.issues.append(kwargs)

    def _get_pure_text(self, line: str) -> str:
        return self.REGEX["code_block"].sub("", line)

    def _classify_line(self, line: str) -> str:
        """ë¼ì¸ì˜ ìœ í˜•ì„ ë¶„ë¥˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not line.strip(): return "BLANK"
        if self.REGEX["passage_header"].match(line): return "PASSAGE_HEADER"
        if self.REGEX["markdown_header"].match(line): return "MARKDOWN_HEADER"
        if self.REGEX["comment"].match(line.strip()): return "COMMENT"
        if not self._get_pure_text(line).strip(): return "PURE_CODE"
        if not self.REGEX["code_block"].search(line): return "PURE_TEXT"
        return "MIXED_CONTENT"

    def run_all_checks(self):
        """ëª¨ë“  ê²€ì¦ ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("\n--- 1ë‹¨ê³„: êµ¬ì¡°ì  ë¬´ê²°ì„± ê²€ì‚¬ ì‹œì‘ ---")
        is_structurally_sound = self._check_line_count_and_structure()
        self._check_core_identifiers()
        
        print("\n--- 2ë‹¨ê³„: êµ¬ë¬¸, í”Œë ˆì´ ê°€ëŠ¥ì„±, ê·œì¹™ ì¤€ìˆ˜ ê²€ì‚¬ ì‹œì‘ ---")
        self._check_global_variable_consistency()
        self._check_all_lines(is_structurally_sound)
        
        print(f"\nëª¨ë“  ê²€ì¦ ì™„ë£Œ. ì´ {len(self.issues)}ê°œì˜ ë¬¸ì œ ë°œê²¬.")

    def _check_line_count_and_structure(self) -> bool:
        if len(self.original_lines) == len(self.translated_lines):
            return True
        self._add_issue(
            line_num=0, severity="CRITICAL", type="êµ¬ì¡°ì  ì˜¤ë¥˜",
            description=f"íŒŒì¼ì˜ ì „ì²´ ì¤„ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì›ë³¸: {len(self.original_lines)}ì¤„, ë²ˆì—­ë³¸: {len(self.translated_lines)}ì¤„)"
        )
        matcher = difflib.SequenceMatcher(None, self.original_lines, self.translated_lines, autojunk=False)
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal' or tag == 'replace': continue
            diff_lines = []
            start = max(0, i1 - self.CONTEXT_LINES)
            for i in range(start, i1): diff_lines.append(f"  {self.original_lines[i]}")
            description = ""
            if tag == 'delete':
                description = f"ì›ë³¸ {i1+1}ë²ˆì§¸ ì¤„ ê·¼ì²˜ì˜ ë‚´ìš©ì´ ë²ˆì—­ë³¸ì—ì„œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                for i in range(i1, i2): diff_lines.append(f"- {self.original_lines[i]}")
            elif tag == 'insert':
                description = f"ë²ˆì—­ë³¸ {j1+1}ë²ˆì§¸ ì¤„ ê·¼ì²˜ì— ì›ë³¸ì— ì—†ëŠ” ë‚´ìš©ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
                for j in range(j1, j2): diff_lines.append(f"+ {self.translated_lines[j]}")
            end = min(len(self.original_lines), i2 + self.CONTEXT_LINES)
            for i in range(i2, end): diff_lines.append(f"  {self.original_lines[i]}")
            self._add_issue(
                line_num=i1 + 1, severity="CRITICAL", type="êµ¬ì¡°ì  ë¶ˆì¼ì¹˜ (ì¤„ ì‚­ì œ/ì¶”ê°€)",
                description=description, diff_text="\n".join(diff_lines)
            )
        return False

    def _check_core_identifiers(self):
        orig_headers = [h for h, l in self._extract_identifiers(self.original_lines, "passage_header")]
        trans_headers = [h for h, l in self._extract_identifiers(self.translated_lines, "passage_header")]
        if orig_headers != trans_headers:
             self._add_issue(line_num=0, severity="CRITICAL", type="íŒ¨ì‹œì§€ í—¤ë” ë¶ˆì¼ì¹˜",
                             description="íŒ¨ì‹œì§€ í—¤ë”ì˜ ìˆœì„œë‚˜ ë‚´ìš©ì´ ì›ë³¸ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ê²Œì„ ë§í¬ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        orig_dests = {d for d, l in self._extract_identifiers(self.original_lines, "link_destination")}
        trans_dests = {d for d, l in self._extract_identifiers(self.translated_lines, "link_destination")}
        if orig_dests != trans_dests:
            missing = orig_dests - trans_dests
            added = trans_dests - orig_dests
            desc = "ë§í¬ ëª©ì ì§€ ëª©ë¡ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            if missing: desc += f" ëˆ„ë½: {', '.join(sorted(list(missing))[:5])} ë“±"
            if added: desc += f" ì¶”ê°€/ì˜¤íƒ€: {', '.join(sorted(list(added))[:5])} ë“±"
            self._add_issue(line_num=0, severity="CRITICAL", type="ë§í¬ ëª©ì ì§€ ë¶ˆì¼ì¹˜", description=desc)

    def _extract_identifiers(self, lines: List[str], id_type: str) -> List[Tuple[str, int]]:
        extracted = []
        for i, line in enumerate(lines):
            if id_type == "passage_header":
                if match := self.REGEX["passage_header"].match(line):
                    extracted.append((match.group(1).strip(), i + 1))
            elif id_type == "link_destination":
                for _, dest in self.REGEX["link_with_dest"].findall(line):
                    extracted.append((dest.strip(), i + 1))
                for dest in self.REGEX["link_simple"].findall(line):
                    if '|' not in dest: extracted.append((dest.strip(), i + 1))
        return extracted

    def _check_global_variable_consistency(self):
        original_vars = set(self.REGEX["variable"].findall("\n".join(self.original_lines)))
        translated_vars = set(self.REGEX["variable"].findall("\n".join(self.translated_lines)))
        if original_vars != translated_vars:
            missing = original_vars - translated_vars
            added = translated_vars - original_vars
            desc = "ì „ì²´ ë³€ìˆ˜ ëª©ë¡ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            if missing: desc += f" ëˆ„ë½ëœ ë³€ìˆ˜: {', '.join(sorted(list(missing))[:5])} ë“±"
            if added: desc += f" ì¶”ê°€/ì†ìƒëœ ë³€ìˆ˜: {', '.join(sorted(list(added))[:5])} ë“±"
            self._add_issue(line_num=0, severity="CRITICAL", type="ì „ì—­ ë³€ìˆ˜ ë¶ˆì¼ì¹˜", description=desc)

    def _check_all_lines(self, is_structurally_sound: bool):
        """ëª¨ë“  ë¼ì¸ì„ ìˆœíšŒí•˜ë©° ìœ í˜•ì— ë§ëŠ” ê²€ì‚¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        for i, translated_line in enumerate(self.translated_lines):
            line_num = i + 1
            original_line = self.original_lines[i] if is_structurally_sound else ""
            
            line_type = self._classify_line(translated_line)
            orig_line_type = self._classify_line(original_line) if is_structurally_sound else None

            # 1. ë²ˆì—­ë˜ë©´ ì•ˆ ë˜ëŠ” ë¼ì¸ ìœ í˜• ê²€ì‚¬
            if line_type in ["PASSAGE_HEADER", "MARKDOWN_HEADER", "COMMENT", "BLANK"]:
                if is_structurally_sound and original_line != translated_line:
                    self._add_issue(severity="CRITICAL", type="ì½”ë“œ ë¼ì¸ ë¶ˆì¼ì¹˜",
                                    description=f"'{line_type}' ìœ í˜•ì˜ ë¼ì¸ì€ ì›ë³¸ê³¼ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.",
                                    line_num=line_num, original=original_line, translated=translated_line)
                continue

            # 2. ì½˜í…ì¸ ê°€ í¬í•¨ëœ ë¼ì¸ ê²€ì‚¬
            if line_type in ["PURE_TEXT", "MIXED_CONTENT"]:
                self._check_links_for_playability(translated_line, line_num)
                self._check_untranslated_content(original_line, translated_line, line_num, is_structurally_sound)
                self._check_forbidden_patterns(translated_line, line_num)
                if is_structurally_sound:
                    self._check_glossary_compliance_nlp(original_line, translated_line, line_num)

            # 3. ëª¨ë“  ë¼ì¸ ëŒ€ìƒ ê²€ì‚¬
            self._check_text_corruption(translated_line, line_num)
            
            # 4. ì½”ë“œ ë¼ì¸ ë¬´ê²°ì„± ê²€ì‚¬ (êµ¬ì¡°ê°€ ê°™ì„ ë•Œë§Œ)
            if is_structurally_sound:
                self._check_macro_corruption(original_line, translated_line, line_num)

    def _check_macro_corruption(self, original_line, translated_line, line_num):
        """ë§¤í¬ë¡œ ë‚´ë¶€ ë¬¸ìì—´ ë¦¬í„°ëŸ´ ë²ˆì—­ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
        original_macros = self.REGEX["macro"].findall(original_line)
        translated_macros = self.REGEX["macro"].findall(translated_line)
        if len(original_macros) == len(translated_macros):
            for orig_macro, trans_macro in zip(original_macros, translated_macros):
                # ë™ì  í‘œí˜„ì‹(ì˜ˆ: "text" + var) ë‚´ë¶€ì˜ ë¬¸ìì—´ë„ ê²€ì‚¬
                content = trans_macro[2:-2].strip()
                if '+' in content:
                    literals = self.REGEX["string_literal"].findall(content)
                else: # ë‹¨ìˆœ ë¬¸ìì—´
                    literals = self.REGEX["string_literal"].findall(trans_macro)

                for literal in literals:
                    if self.REGEX["korean"].search(literal) and literal not in self.ALLOWED_POSTPOSITIONS:
                        self._add_issue(
                            severity="CRITICAL", type="ë§¤í¬ë¡œ ì½”ë“œ ì†ìƒ",
                            description=f"ë§¤í¬ë¡œ ë‚´ë¶€ ì½”ë“œ ì‹ë³„ì(ë¬¸ìì—´) '{literal}'ì´(ê°€) ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤.",
                            line_num=line_num, original=f"`{orig_macro}`", translated=f"`{trans_macro}`"
                        )
                        break

    def _check_links_for_playability(self, line, line_num):
        """ë§í¬ì˜ í‘œì‹œ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë²ˆì—­ë˜ì§€ ì•Šì•˜ëŠ”ì§€ ê²€ì‚¬í•©ë‹ˆë‹¤."""
        all_links = self.REGEX["link_with_dest"].findall(line) + [(m, m) for m in self.REGEX["link_simple"].findall(line) if '|' not in m]
        for display_text, dest in all_links:
            if not display_text.strip():
                self._add_issue(severity="WARNING", type="ë¹ˆ ìƒí˜¸ì‘ìš©",
                                description="í”Œë ˆì´ì–´ê°€ í´ë¦­í•  ìˆ˜ ì—†ëŠ” 'ë¹ˆ ë§í¬'ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                                line_num=line_num, translated=line)
            elif self.REGEX["english_only"].match(self._get_pure_text(display_text)):
                word_count = len(self.REGEX["word_tokenizer"].findall(display_text))
                severity = "WARNING" if word_count >= self.UNTRANSLATED_LINK_WORD_THRESHOLD else "INFO"
                self._add_issue(severity=severity, type="ë¯¸ë²ˆì—­ ì˜ì‹¬ (ë§í¬)",
                                description=f"ë§í¬ í‘œì‹œ í…ìŠ¤íŠ¸ '{display_text}'ì´(ê°€) ë²ˆì—­ë˜ì§€ ì•Šì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
                                line_num=line_num, translated=line)

    def _check_untranslated_content(self, original_line, translated_line, line_num, is_structurally_sound):
        """ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë¼ì¸ì´ ë²ˆì—­ë˜ì§€ ì•Šì•˜ëŠ”ì§€ ë¹„ìœ¨ ê¸°ë°˜ìœ¼ë¡œ ê²€ì‚¬í•©ë‹ˆë‹¤."""
        pure_translated = self._get_pure_text(translated_line)
        if not pure_translated.strip() or self.REGEX["korean"].search(pure_translated):
            return
        
        words = self.REGEX["word_tokenizer"].findall(pure_translated)
        if not words: return

        english_words = sum(1 for word in words if self.REGEX["english_only"].match(word) and not word.isdigit())
        if (english_words / len(words)) >= self.ENGLISH_RATIO_THRESHOLD:
            severity = "WARNING" if is_structurally_sound and original_line.strip() == translated_line.strip() else "INFO"
            desc = "ì´ ë¼ì¸ì€ ë²ˆì—­ì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜(ì›ë³¸ê³¼ ë™ì¼), ëŒ€ë¶€ë¶„ì´ ì˜ì–´ë¡œ êµ¬ì„±ë˜ì–´ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            self._add_issue(
                severity=severity, type="ë¯¸ë²ˆì—­ ì˜ì‹¬ (ì½˜í…ì¸ )", description=desc,
                line_num=line_num, original=original_line, translated=translated_line
            )

    def _check_text_corruption(self, line, line_num):
        """í…ìŠ¤íŠ¸ ê¹¨ì§ í˜„ìƒì„ íƒì§€í•©ë‹ˆë‹¤."""
        if self.REGEX["corrupted_char"].search(line):
            self._add_issue(
                severity="CRITICAL", type="í…ìŠ¤íŠ¸ ì†ìƒ",
                description="íŒŒì¼ ì¸ì½”ë”© ë¬¸ì œë¡œ ì¸í•´ ê¹¨ì§„ ë¬¸ì(ï¿½)ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                line_num=line_num, translated=line
            )

    def _check_forbidden_patterns(self, line, line_num):
        """ê¸ˆì§€ëœ ë²ˆì—­ íŒ¨í„´(ì›ë¬¸ ë³‘ê¸°)ì„ íƒì§€í•©ë‹ˆë‹¤."""
        if self.REGEX["forbidden_pattern"].search(line):
            self._add_issue(
                severity="WARNING", type="ê¸ˆì§€ëœ íŒ¨í„´ ì‚¬ìš©",
                description="ë²ˆì—­ë¬¸ ë’¤ì— ê´„í˜¸ë¥¼ ì‚¬ìš©í•œ ì›ë¬¸ ë³‘ê¸° íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                line_num=line_num, translated=line
            )

    def _check_glossary_compliance_nlp(self, original_line, translated_line, line_num):
        """NLPì™€ Aho-Corasickì„ ì‚¬ìš©í•˜ì—¬ ìš©ì–´ì§‘ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ ì •ë°€ ê²€ì‚¬í•©ë‹ˆë‹¤."""
        if not self.glossary: return

        pure_original = self._get_pure_text(original_line)
        if not pure_original.strip(): return

        found_eng_terms = {item[1][0] for item in self.glossary_automaton.iter(pure_original)}
        if not found_eng_terms: return

        pure_translated = self._get_pure_text(translated_line)
        tokens = self.kiwi.tokenize(pure_translated)
        found_kor_tokens = {token.form for token in tokens}

        for eng_key in found_eng_terms:
            kor_value = self.glossary.get(eng_key)
            if not kor_value: continue

            if kor_value not in found_kor_tokens and kor_value not in pure_translated:
                if eng_key.lower() in pure_translated.lower():
                    self._add_issue(
                        severity="INFO", type="ìš©ì–´ì§‘ ë¯¸ì ìš©",
                        description=f"ìš©ì–´ì§‘ ë‹¨ì–´ '{eng_key}'ê°€ ë²ˆì—­ë˜ì§€ ì•Šê³  ì›ë¬¸ì— ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.",
                        line_num=line_num, original=original_line, translated=translated_line
                    )
                else:
                    self._add_issue(
                        severity="WARNING", type="ìš©ì–´ì§‘ ì˜¤ì—­/ëˆ„ë½ ì˜ì‹¬",
                        description=f"ìš©ì–´ì§‘ ë‹¨ì–´ '{eng_key}'ì˜ ë²ˆì—­ '{kor_value}'ì´(ê°€) ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ë²ˆì—­ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
                        line_num=line_num, original=original_line, translated=translated_line
                    )

    def generate_report(self, output_path: Path):
        """ê²€ì¦ ê²°ê³¼ë¥¼ Markdown íŒŒì¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.issues:
            report_content = f"# âœ… ê²€ì¦ ì™„ë£Œ: {self.translated_path.name}\n\n**ì¶•í•˜í•©ë‹ˆë‹¤! ë°œê²¬ëœ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.**"
        else:
            severity_order = {"CRITICAL": 0, "WARNING": 1, "INFO": 2}
            sorted_issues = sorted(self.issues, key=lambda x: (severity_order.get(x["severity"], 99), x["line_num"]))
            
            summary = defaultdict(int)
            for issue in self.issues: summary[issue["severity"]] += 1

            report_content = f"# â— ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸: {self.translated_path.name}\n\n"
            report_content += f"## ìš”ì•½\n\n- **ì´ ë¬¸ì œ ìˆ˜: {len(self.issues)}**\n"
            if summary["CRITICAL"] > 0: report_content += f"- ğŸ”´ **ì¹˜ëª…ì  ì˜¤ë¥˜ (CRITICAL): {summary['CRITICAL']}**\n"
            if summary["WARNING"] > 0: report_content += f"- ğŸŸ¡ **ê²½ê³  (WARNING): {summary['WARNING']}**\n"
            if summary["INFO"] > 0: report_content += f"- ğŸ”µ **ì •ë³´ (INFO): {summary['INFO']}**\n"
            
            report_content += "\n---\n\n## ìƒì„¸ ë‚´ìš©\n\n"

            for issue in sorted_issues:
                icon = {"CRITICAL": "ğŸ”´", "WARNING": "ğŸŸ¡", "INFO": "ğŸ”µ"}.get(issue["severity"], "âšªï¸")
                line_info = f"(ì›ë³¸ ê¸°ì¤€ Line: {issue['line_num']})" if issue['line_num'] > 0 else "(ì „ì—­ ê²€ì‚¬)"
                report_content += f"### {icon} [{issue['severity']}] {issue['type']} {line_info}\n\n"
                report_content += f"- **ë¬¸ì œ ì„¤ëª…:** {issue['description']}\n"
                if issue.get('diff_text'):
                    report_content += f"\n**ì°¨ì´ì  ë¶„ì„ (Diff):**\n```diff\n{issue['diff_text']}\n```\n"
                if issue.get('original'):
                    report_content += f"- **ì›ë³¸:** `{issue.get('original')}`\n"
                if issue.get('translated'):
                    report_content += f"- **ë²ˆì—­ë³¸:** `{issue.get('translated')}`\n"
                report_content += "\n---\n"
        
        output_path.write_text(report_content, 'utf-8')
        print(f"\në¦¬í¬íŠ¸ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    # --- ì„¤ì •: ì—¬ê¸°ì— ê²€ì¦í•  íŒŒì¼ ê²½ë¡œë¥¼ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”. ---
    ORIGINAL_FILE_PATH =
    TRANSLATED_FILE_PATH = 
    OUTPUT_REPORT_PATH = 
    GLOSSARY_FILE_PATH =
    # ----------------------------------------------------

    original_p = Path(ORIGINAL_FILE_PATH)
    translated_p = Path(TRANSLATED_FILE_PATH)
    glossary_p = Path(GLOSSARY_FILE_PATH) if GLOSSARY_FILE_PATH and Path(GLOSSARY_FILE_PATH).exists() else None
    output_p = Path(OUTPUT_REPORT_PATH)

    validator = TweeL10nValidator(original_path=original_p, translated_path=translated_p, glossary_path=glossary_p)
    validator.run_all_checks()
    validator.generate_report(output_path=output_p)



